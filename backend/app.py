from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel, Field
from typing import List, Optional

from pathlib import Path
import shutil
import base64
from io import BytesIO
from fastapi.responses import PlainTextResponse


# === Import Model Logic ===
from model.read_summarize.mvp_reader import (
    hybrid_retrieve,
    build_answer_prompt,
    gpt4omini_chat,
    cmd_ingest
)

pipe = None


app = FastAPI(
    title="ğŸ“š ReadingMate API",
    description="Hybrid Retrieval + GPT + Stable Diffusion Backend",
    version="1.0.0"
)


# --- CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡ íŠ¸ ë°°í¬ ì‹œ ë„ë©”ì¸ë§Œ í—ˆìš© ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =========================================================
# ğŸ“Œ Pydantic Schemas
# =========================================================
class AskRequest(BaseModel):
    doc_id: str = Field(..., example="ìš´ìˆ˜ì¢‹ì€ë‚ ")
    question: str = Field(..., example="ì£¼ì¸ê³µì€ ê²°êµ­ ì–´ë””ë¡œ ê°”ë‚˜ìš”?")
    k: int = Field(6, description="ê²€ìƒ‰í•  ë¬¸ë§¥ ìˆ˜")


class AskResponse(BaseModel):
    answer: str
    retrieved_chunks: List[str]
    scores: List[float]


class SummarizeRequest(BaseModel):
    doc_id: str = Field(..., example="ìš´ìˆ˜ì¢‹ì€ë‚ ")
    sentences: int = Field(5)


class SummarizeResponse(BaseModel):
    summary: str


class GenerateImageRequest(BaseModel):
    prompt: str = Field(..., example="rainy alley in seoul, watercolor style")
    steps: int = Field(40)


class GenerateImageResponse(BaseModel):
    preview_base64: str


# =========================================================
# 1ï¸âƒ£ Document Ingest
# =========================================================
@app.post("/ingest", tags=["ğŸ“„ Document"])
async def ingest(doc_id: str = Form(...), file: UploadFile = File(...)):
    """
    ì—…ë¡œë“œí•œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¶„í• /ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ DBë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    temp_path = Path("temp_upload.txt")

    with open(temp_path, "wb") as f:
        shutil.copyfileobj(file.file, f)

    class NS: pass
    ns = NS()
    ns.doc_id = doc_id
    ns.path = str(temp_path)
    ns.unit = "para"
    ns.window = 1
    ns.stride = 1

    try:
        cmd_ingest(ns)
        return {"status": "success", "doc_id": doc_id}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# 2ï¸âƒ£ Ask (RAG + GPT)
# =========================================================
@app.post("/ask", response_model=AskResponse, tags=["ğŸ¤– Q&A"])
async def ask(request: AskRequest):
    """ë¬¸ì„œ ê¸°ë°˜ ì‹¤ì‹œê°„ Retrieval + GPT reasoning"""
    try:
        # ğŸ‘‰ ì§ˆë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ 400 ì—ëŸ¬ ì‘ë‹µ
        if not request.question.strip():
            raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # ğŸ‘‰ ë¬¸ì„œ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        storage_path = Path("model/read_summarize/storage") / request.doc_id
        if not storage_path.exists():
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œ ID '{request.doc_id}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ” ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ ìƒì„± + GPT í˜¸ì¶œ
        ids, scores, chunks = hybrid_retrieve(request.doc_id, request.question, k=request.k)
        prompt = build_answer_prompt(request.question, chunks)
        answer = gpt4omini_chat(prompt)

        return AskResponse(answer=answer, retrieved_chunks=chunks, scores=scores)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# =========================================================
# 3ï¸âƒ£ Summarization
# =========================================================
@app.post("/summarize", response_model=SummarizeResponse, tags=["ğŸ“Œ Summary"])
async def summarize(request: SummarizeRequest):
    """ì „ì²´ ë¬¸ì„œ ê¸°ë°˜ GPT ìš”ì•½ ìƒì„±"""
    try:
        _, _, chunks = hybrid_retrieve(request.doc_id, "ì „ì²´ ì¤„ê±°ë¦¬")
        text = "\n".join(chunks)

        prompt = f"ì•„ë˜ ë‚´ìš©ì„ {request.sentences} ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{text}"
        answer = gpt4omini_chat(prompt)

        return SummarizeResponse(summary=answer)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# 4ï¸âƒ£ Image Generation (Lazy Stable Diffusion)
# =========================================================
@app.post("/generate", response_model=GenerateImageResponse, tags=["ğŸ¨ Image"])
async def generate(prompt: str = Form(...), steps: int = Form(30)):
    """ì…ë ¥ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±"""

    global pipe
    from diffusers import StableDiffusionPipeline, DDIMScheduler
    import torch

    try:
        # ìµœì´ˆ ìš”ì²­ ì‹œ ë¡œë”©
        if pipe is None:
            print("ğŸš€ Loading Stable Diffusion...")
            model_path = "./model/generate/models/stable_diffusion"
            pipe = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                safety_checker=None
            )
            pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
            pipe.to("cpu")
            print("âœ… Ready.")

        img = pipe(prompt, num_inference_steps=steps).images[0]

        buffer = BytesIO()
        img.save(buffer, format="PNG")
        img_str = base64.b64encode(buffer.getvalue()).decode()

        return GenerateImageResponse(preview_base64=img_str)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


BASE_DIR = Path(__file__).resolve().parent
BOOK_DIR = BASE_DIR / "model" / "read_summarize"

@app.get("/api/book/{book_id}")
def get_book(book_id: str):
    # print(f"ğŸ“– ìš”ì²­ ë“¤ì–´ì˜¨ book_id: {book_id}")
    book_path = BOOK_DIR / f"{book_id}.txt"
    # print(f"â¡ï¸ ì°¾ëŠ” íŒŒì¼ ê²½ë¡œ: {book_path}")

    try:
        with open(book_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
    
# =========================================================
# Health Check
# =========================================================
@app.get("/", tags=["ğŸ©º Health"])
async def root():
    return {"message": "ğŸš€ ReadingMate API is running!"}
