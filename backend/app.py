from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel, Field
from typing import List, Optional

from pathlib import Path
import shutil
import base64
from io import BytesIO
from fastapi.responses import PlainTextResponse
import torch


# === Import Model Logic ===
from model.read_summarize.mvp_reader import (
    hybrid_retrieve,
    build_answer_prompt,
    gpt4omini_chat,
    cmd_ingest
)

pipe = None
sd_device = None

BASE_DIR = Path(__file__).resolve().parent
BOOK_DIR = BASE_DIR / "model" / "read_summarize"
SD_MODEL_PATH = BASE_DIR / "model" / "generate" / "models" / "stable_diffusion"

app = FastAPI(
    title="ğŸ“š ReadingMate API",
    description="Hybrid Retrieval + GPT + Stable Diffusion Backend",
    version="1.0.0"
)


# --- CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡ íŠ¸ ë°°í¬ ì‹œ ë„ë©”ì¸ë§Œ í—ˆìš© ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =========================================================
# ğŸ“Œ Pydantic Schemas
# =========================================================
class AskRequest(BaseModel):
    doc_id: str = Field(..., example="ìš´ìˆ˜ì¢‹ì€ë‚ ")
    question: str = Field(..., example="ì£¼ì¸ê³µì€ ê²°êµ­ ì–´ë””ë¡œ ê°”ë‚˜ìš”?")
    k: int = Field(6, description="ê²€ìƒ‰í•  ë¬¸ë§¥ ìˆ˜")


class AskResponse(BaseModel):
    answer: str
    retrieved_chunks: List[str]
    scores: List[float]


class SummarizeRequest(BaseModel):
    doc_id: str = Field(..., example="ìš´ìˆ˜ì¢‹ì€ë‚ ")
    sentences: int = Field(5)


class SummarizeResponse(BaseModel):
    summary: str


class GenerateImageRequest(BaseModel):
    prompt: str = Field(..., example="rainy alley in seoul, watercolor style")
    steps: int = Field(40)


class GenerateImageResponse(BaseModel):
    preview_base64: str


class QuickSummaryRequest(BaseModel):
  text: str = Field(..., description="ìš”ì•½í•  ì„ íƒ í…ìŠ¤íŠ¸")
  sentences: int = Field(2, description="ìš”ì•½ ë¬¸ì¥ ìˆ˜")


class QuickSummaryResponse(BaseModel):
  summary: str


# =========================================================
# 1ï¸âƒ£ Document Ingest
# =========================================================
@app.post("/ingest", tags=["ğŸ“„ Document"])
async def ingest(doc_id: str = Form(...), file: UploadFile = File(...)):
    """
    ì—…ë¡œë“œí•œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¶„í• /ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ DBë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    temp_path = Path("temp_upload.txt")

    with open(temp_path, "wb") as f:
        shutil.copyfileobj(file.file, f)

    class NS: pass
    ns = NS()
    ns.doc_id = doc_id
    ns.path = str(temp_path)
    ns.unit = "para"
    ns.window = 1
    ns.stride = 1

    try:
        cmd_ingest(ns)
        return {"status": "success", "doc_id": doc_id}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# 2ï¸âƒ£ Ask (RAG + GPT)
# =========================================================
@app.post("/ask", response_model=AskResponse, tags=["ğŸ¤– Q&A"])
async def ask(request: AskRequest):
    """ë¬¸ì„œ ê¸°ë°˜ ì‹¤ì‹œê°„ Retrieval + GPT reasoning"""
    try:
        # ğŸ‘‰ ì§ˆë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ 400 ì—ëŸ¬ ì‘ë‹µ
        if not request.question.strip():
            raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        # ğŸ‘‰ ë¬¸ì„œ IDê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        storage_path = Path("model/read_summarize/storage") / request.doc_id
        if not storage_path.exists():
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œ ID '{request.doc_id}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ” ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ ìƒì„± + GPT í˜¸ì¶œ
        ids, scores, chunks = hybrid_retrieve(request.doc_id, request.question, k=request.k)
        prompt = build_answer_prompt(request.question, chunks)
        answer = gpt4omini_chat(prompt)

        return AskResponse(answer=answer, retrieved_chunks=chunks, scores=scores)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# =========================================================
# 3ï¸âƒ£ Summarization
# =========================================================
@app.post("/summarize", response_model=SummarizeResponse, tags=["ğŸ“Œ Summary"])
async def summarize(request: SummarizeRequest):
    """ì „ì²´ ë¬¸ì„œ ê¸°ë°˜ GPT ìš”ì•½ ìƒì„±"""
    try:
        _, _, chunks = hybrid_retrieve(request.doc_id, "ì „ì²´ ì¤„ê±°ë¦¬")
        text = "\n".join(chunks)

        prompt = f"ì•„ë˜ ë‚´ìš©ì„ {request.sentences} ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{text}"
        answer = gpt4omini_chat(prompt)

        return SummarizeResponse(summary=answer)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# 3-1ï¸âƒ£ Selected Text Quick Summary
# =========================================================
@app.post("/summarize_text", response_model=QuickSummaryResponse, tags=["ğŸ“Œ Summary"])
async def summarize_text(request: QuickSummaryRequest):
    """ì„ íƒëœ ì§§ì€ í…ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ìš”ì•½"""
    try:
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        prompt = (
            f"ë‹¤ìŒ ê¸€ì„ {request.sentences}ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜.\n\n"
            f"{request.text}"
        )
        answer = gpt4omini_chat(prompt)

        return QuickSummaryResponse(summary=answer)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# =========================================================
# 4ï¸âƒ£ Image Generation (Lazy Stable Diffusion)
# =========================================================
@app.post("/generate", response_model=GenerateImageResponse, tags=["ğŸ¨ Image"])
async def generate(prompt: str = Form(...), steps: int = Form(60)):
    """ì…ë ¥ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±"""

    global pipe
    global sd_device
    from diffusers import StableDiffusionPipeline, DDIMScheduler

    try:
        # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ ì„ ì²´í¬ (ë„¤íŠ¸ì›Œí¬ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
        model_index = SD_MODEL_PATH / "model_index.json"
        if not model_index.exists():
            return JSONResponse(
                status_code=503,
                content={
                    "error": "Stable Diffusion ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. "
                             f"{SD_MODEL_PATH}ì— ëª¨ë¸ íŒŒì¼ì„ ë°°ì¹˜í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                },
            )

        # ìµœì´ˆ ìš”ì²­ ì‹œ ë¡œë”©
        if pipe is None:
            print(f"ğŸš€ Loading Stable Diffusion from {SD_MODEL_PATH} ...")
            # ë””ë°”ì´ìŠ¤ ì„ íƒ
            if torch.backends.mps.is_available():
                sd_device = torch.device("mps")
                dtype = torch.float16
                print("âœ… Using Apple MPS (float16)")
            else:
                sd_device = torch.device("cpu")
                dtype = torch.float32
                print("âœ… Using CPU (float32)")

            pipe = StableDiffusionPipeline.from_pretrained(
                str(SD_MODEL_PATH),
                torch_dtype=dtype,
                safety_checker=None,
                local_files_only=True,
            )
            pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
            pipe.to(sd_device)
            print("âœ… Stable Diffusion Ready.")

        img = pipe(prompt, num_inference_steps=steps).images[0]

        buffer = BytesIO()
        img.save(buffer, format="PNG")
        img_str = base64.b64encode(buffer.getvalue()).decode()

        return GenerateImageResponse(preview_base64=img_str)

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


BASE_DIR = Path(__file__).resolve().parent
BOOK_DIR = BASE_DIR / "model" / "read_summarize"

@app.get("/api/book/{book_id}")
def get_book(book_id: str):
    # print(f"ğŸ“– ìš”ì²­ ë“¤ì–´ì˜¨ book_id: {book_id}")
    book_path = BOOK_DIR / f"{book_id}.txt"
    # print(f"â¡ï¸ ì°¾ëŠ” íŒŒì¼ ê²½ë¡œ: {book_path}")

    try:
        with open(book_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        return f"íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
    
# =========================================================
# Health Check
# =========================================================
@app.get("/", tags=["ğŸ©º Health"])
async def root():
    return {"message": "ğŸš€ ReadingMate API is running!"}
