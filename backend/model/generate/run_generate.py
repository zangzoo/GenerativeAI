import os
import torch
import gradio as gr
from diffusers import StableDiffusionPipeline, DDIMScheduler
from transformers import CLIPTokenizer

# =====================================
# ğŸ”¥ Device ì„¤ì • (MPS / CPU ì„ íƒ ê°€ëŠ¥)
# =====================================
FORCE_CPU = True  # âœ… CPUë¡œë§Œ ëŒë¦¬ê³  ì‹¶ìœ¼ë©´ True ë¡œ ë°”ê¾¸ê¸°

if FORCE_CPU:
    device = "cpu"
else:
    device = "mps" if torch.backends.mps.is_available() else "cpu"

print("Using device:", device)

# dtype ì„¤ì • (CPUëŠ” float32, MPSëŠ” float16)
dtype = torch.float16 if device == "mps" else torch.float32

# ğŸ”¹ Stable Diffusion ëª¨ë¸ ë¡œë“œ
model_path = "/Users/zangzoo/vscode/ReadingMate/backend/model/generate/models/stable_diffusion"

pipe = StableDiffusionPipeline.from_pretrained(
    model_path,
    torch_dtype=dtype,
    safety_checker=None
)

# ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
pipe.to(device)

# DDIM ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ êµì²´
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)

# ğŸ”¹ CLIP tokenizer ë¡œë“œ
tokenizer = CLIPTokenizer.from_pretrained(model_path + "/tokenizer", from_slow=True)

# ğŸ”¹ ê¸´ ë¬¸ì¥ ìë™ chunk í•¨ìˆ˜
def split_long_prompt(text, max_tokens=75):
    tokens = tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        token_chunk = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(token_chunk)
        chunks.append(chunk_text)

    return chunks

# ================================
# ğŸ¨ ìƒì„± í•¨ìˆ˜ (Gradioì—ì„œ í˜¸ì¶œ)
# ================================
def generate_image(user_prompt, steps):
    try:
        prompt_chunks = split_long_prompt(user_prompt)
        final_prompt = ", ".join(prompt_chunks)

        # CPUì—ì„œë„ ë™ì¼í•˜ê²Œ í˜¸ì¶œ ê°€ëŠ¥
        with torch.autocast("cpu" if device == "cpu" else "mps", enabled=(device != "cpu")):
            image = pipe(final_prompt, num_inference_steps=steps).images[0]

        return image

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ================================
# ğŸŒ Gradio UI
# ================================
with gr.Blocks(title="ReadingMate Image Generator") as demo:
    gr.Markdown("## ğŸ¨ ReadingMate Stable Diffusion ì´ë¯¸ì§€ ìƒì„±ê¸°")
    gr.Markdown("ê¸´ ë¬¸ì¥ì„ ë„£ì–´ë„ ìë™ìœ¼ë¡œ chunkingí•´ì„œ ìƒì„±í•©ë‹ˆë‹¤!")

    with gr.Row():
        with gr.Column(scale=2):
            user_prompt = gr.Textbox(
                label="í”„ë¡¬í”„íŠ¸ ì…ë ¥",
                placeholder="ì—¬ê¸°ì— ê¸´ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”...",
                lines=4
            )
            steps = gr.Slider(20, 80, value=40, step=5, label="Inference Steps")

            submit_btn = gr.Button("ì´ë¯¸ì§€ ìƒì„±!")

        with gr.Column(scale=3):
            output_image = gr.Image(label="ê²°ê³¼ ì´ë¯¸ì§€", type="pil")

    submit_btn.click(
        fn=generate_image,
        inputs=[user_prompt, steps],
        outputs=[output_image]
    )

# Run
demo.launch(server_name="0.0.0.0", server_port=7860)
