#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG MVP: Hybrid Search (BM25 + Dense) + Llama
- BM25: í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ (sparse)
- Dense: ì˜ë¯¸ ìœ ì‚¬ë„ (FAISS)
- Hybrid: ë‘ ì ìˆ˜ ê²°í•© â†’ ë” ì •í™•í•œ retrieval

ì„¤ì¹˜:
  pip install rank-bm25 faiss-cpu sentence-transformers transformers torch --upgrade python-dotenv numpy openai gradio


ì˜ˆì‹œ:
  .venv/Scripts/activate 
  (1)cli ëª¨ë“œ  ... doc_idëŠ” ì„ì˜ ì§€ì • ê°€ëŠ¥, txt íŒŒì¼ ê²½ë¡œ ì§€ì •
  python backend/model/read_summarize/mvp_reader.py ingest --doc_id rng --path backend/model/read_summarize/romeoandjuliet.txt --unit para --window 2 --stride 1
  python backend/model/read_summarize/mvp_reader.py ask --doc_id rng -q "ë¡œë¯¸ì˜¤ëŠ” ì™œ ì£½ì—ˆì–´?" -k 6
  python backend/model/read_summarize/mvp_reader.py summarize --doc_id rng --sentences 7


  python mvp_reader.py ingest --doc_id novel --path luckyday.txt --unit para
  python mvp_reader.py ask --doc_id novel -q "ì£¼ì¸ê³µì€ ì–´ë”” ê°”ì–´?" -k 6
  python mvp_reader.py summarize --doc_id novel --sentences 7

  (2)gradio UI ëª¨ë“œ
  python backend/model/read_summarize/mvp_reader.py ui

  python mvp_reader.py ui

  ë¡œë¯¸ì˜¤ëŠ” ë¬´ìŠ¨ ê°€ë¬¸ì˜ ë”¸ì´ì—ˆì§€?
  í‹°ë³¼íŠ¸ ì£½ì—ˆì–´? ì¤„ë¦¬ì—£ê³¼ ë¬´ìŠ¨ ì‚¬ì´ê¸¸ë˜ ìŠ¬í¼í•˜ì§€?
  ë¨¸íë¦¬ ì£½ì—ˆì–´? ë¡œë¯¸ì˜¤ì™€ ë¬´ìŠ¨ ì‚¬ì´ê¸¸ë˜ ìŠ¬í¼í•˜ì§€?
  ë¡œë¯¸ì˜¤ëŠ” ì™œ ì¶”ë°©ëì–´?
  

1ï¸âƒ£ ê¹€ ì²¨ì§€ëŠ” ì™œ ì˜¤ëŠ˜ì„ â€œìš´ìˆ˜ ì¢‹ì€ ë‚ â€ì´ë¼ê³  ìƒê°í–ˆë‚˜ìš”?
2ï¸âƒ£ ê¹€ ì²¨ì§€ê°€ ì„¤ë íƒ•ì„ ì‚¬ê°€ë ¤ í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
3ï¸âƒ£ ê¹€ ì²¨ì§€ì˜ ì•„ë‚´ê°€ ë³‘ì´ ì•…í™”ëœ ì›ì¸ì€ ë¬´ì—‡ì´ë¼ê³  ë‚˜ì˜¤ë‚˜ìš”?
4ï¸âƒ£ ê¹€ ì²¨ì§€ê°€ ì§‘ì— ëŒì•„ê°€ê¸° ì‹«ì–´í–ˆë˜ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
5ï¸âƒ£ ë§ˆì§€ë§‰ ì¥ë©´ì—ì„œ ê¹€ ì²¨ì§€ëŠ” ì™œ ìš¸ë‹¤ê°€ ì›ƒë‹¤ê°€ ë°˜ë³µí•˜ë‚˜ìš”?
"""

from __future__ import annotations
import argparse, os, re, json, pickle
from pathlib import Path
from dataclasses import dataclass

from typing import List, Tuple

from dotenv import load_dotenv
load_dotenv()  # .env íŒŒì¼ ìë™ ë¡œë“œ


import numpy as np
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ---------- ì €ì¥ ë£¨íŠ¸ ----------
STORAGE_ROOT = Path(__file__).parent / "storage"

# ---------- ì„ë² ë”© ëª¨ë¸ ----------
_EMB_MODEL_NAME = os.getenv("EMB_MODEL", "dragonkue/BGE-m3-ko")


# ---------- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ ----------
try:
    import faiss
except ImportError as e:
    raise SystemExit("[ERROR] pip install faiss-cpu") from e

try:
    from rank_bm25 import BM25Okapi
except ImportError as e:
    raise SystemExit("[ERROR] pip install rank-bm25") from e

# =========================================================
# ìœ í‹¸
# =========================================================
def read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")

def split_sentences(text: str) -> List[str]:
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"\u3000|\xa0", " ", text)
    parts = re.split(r"(?<=[\.!?ï¼Ÿï¼ã€‚â€¦])\s+|\n+", text)
    return [p.strip() for p in parts if p and p.strip()]

def split_paragraphs(text: str) -> List[str]:
    paras = re.split(r"\n{2,}", text.replace("\r\n", "\n"))
    return [p.strip() for p in paras if p.strip()]

def make_chunks(text: str, unit: str = "para", window:int=1, stride:int=1) -> List[str]:
    items = split_paragraphs(text) if unit == "para" else split_sentences(text)
    if window <= 1: 
        return items
    out = []
    i, n = 0, len(items)
    while i < n:
        j = min(n, i+window)
        out.append(" ".join(items[i:j]))
        if j == n: break
        i += max(1, stride)
    return out

# =========================================================
# í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (BM25ìš©)
# =========================================================
def simple_tokenize(text: str) -> List[str]:
    """ê°„ë‹¨í•œ í•œêµ­ì–´/ì˜ì–´ í† í¬ë‚˜ì´ì € (í˜•íƒœì†Œ ë¶„ì„ ì—†ì´)"""
    # ê³µë°± + íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€ ë¶„ë¦¬
    text = re.sub(r'[^\w\s]', ' ', text)
    tokens = text.lower().split()
    # í•œê¸€ì€ ìŒì ˆ ë‹¨ìœ„ë¡œë„ ì¶”ê°€ (ì§§ì€ ë‹¨ì–´ ë§¤ì¹­ ê°•í™”)
    result = []
    for t in tokens:
        result.append(t)
        if re.search(r'[ê°€-í£]', t) and len(t) > 1:
            result.extend(list(t))  # ìŒì ˆ ë¶„ë¦¬
    return result

# =========================================================
# ì €ì¥ ìŠ¤í‚¤ë§ˆ
# =========================================================
@dataclass
class RAGStore:
    doc_id: str
    chunks: List[str]
    emb_dim: int
    index_path: Path
    bm25_path: Path
    meta_path: Path

    @property
    def base_dir(self) -> Path:
        return STORAGE_ROOT / self.doc_id

    def save_meta(self):
        self.base_dir.mkdir(parents=True, exist_ok=True)
        meta = {"doc_id": self.doc_id, "emb_dim": self.emb_dim, "chunks": len(self.chunks)}
        self.meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
        with open(self.base_dir / "chunks.pkl", "wb") as f:
            pickle.dump(self.chunks, f)

    @staticmethod
    def load(doc_id: str) -> "RAGStore":
        base = STORAGE_ROOT / doc_id
        if not base.exists():
            raise FileNotFoundError(f"[RAG] storage not found: {base}")
        with open(base / "chunks.pkl", "rb") as f:
            chunks = pickle.load(f)
        meta = json.loads((base / "meta.json").read_text(encoding="utf-8"))
        return RAGStore(
            doc_id=doc_id,
            chunks=chunks,
            emb_dim=meta["emb_dim"],
            index_path=base / "faiss.index",
            bm25_path=base / "bm25.pkl",
            meta_path=base / "meta.json",
        )

# =========================================================
# Dense ì„ë² ë”© (FAISS)
# =========================================================
_emb_model = None
def get_emb_model():
    global _emb_model
    if _emb_model is None:
        from sentence_transformers import SentenceTransformer
        _emb_model = SentenceTransformer(_EMB_MODEL_NAME)
    return _emb_model

def embed_texts(texts: List[str]) -> np.ndarray:
    model = get_emb_model()
    vecs = model.encode(texts, batch_size=64, convert_to_numpy=True, normalize_embeddings=True)
    return vecs.astype("float32")

def build_faiss_index(vectors: np.ndarray) -> faiss.IndexFlatIP:
    dim = vectors.shape[1]
    idx = faiss.IndexFlatIP(dim)
    idx.add(vectors)
    return idx

def save_faiss(index: faiss.Index, path: Path):
    faiss.write_index(index, str(path))

def load_faiss(path: Path) -> faiss.Index:
    return faiss.read_index(str(path))

# =========================================================
# Sparse ê²€ìƒ‰ (BM25)
# =========================================================
def build_bm25_index(chunks: List[str]) -> BM25Okapi:
    tokenized = [simple_tokenize(c) for c in chunks]
    return BM25Okapi(tokenized)

def save_bm25(bm25: BM25Okapi, path: Path):
    with open(path, "wb") as f:
        pickle.dump(bm25, f)

def load_bm25(path: Path) -> BM25Okapi:
    with open(path, "rb") as f:
        return pickle.load(f)

# =========================================================
# Hybrid Retrieval
# =========================================================
def hybrid_retrieve(doc_id: str, query: str, k: int = 6, 
                   alpha: float = 0.5) -> Tuple[List[int], List[float], List[str]]:
    """
    Hybrid search: BM25 + Dense
    alpha: BM25 ê°€ì¤‘ì¹˜ (0~1), 1-alpha: Dense ê°€ì¤‘ì¹˜
    alpha=0.5: ê· í˜•, alpha=0.7: BM25 ì¤‘ì‹œ, alpha=0.3: Dense ì¤‘ì‹œ
    """
    store = RAGStore.load(doc_id)
    
    # 1. BM25 ì ìˆ˜
    bm25 = load_bm25(store.bm25_path)
    query_tokens = simple_tokenize(query)
    bm25_scores = bm25.get_scores(query_tokens)
    bm25_scores = np.array(bm25_scores)
    
    # 2. Dense ì ìˆ˜ (FAISS)
    idx = load_faiss(store.index_path)
    qv = embed_texts([query])
    dense_sims, dense_ids = idx.search(qv, len(store.chunks))  # ì „ì²´ ê²€ìƒ‰
    dense_scores = np.zeros(len(store.chunks))
    for i, (chunk_id, sim) in enumerate(zip(dense_ids[0], dense_sims[0])):
        dense_scores[chunk_id] = sim
    
    # 3. ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ)
    if bm25_scores.max() > 0:
        bm25_scores = bm25_scores / bm25_scores.max()
    if dense_scores.max() > 0:
        dense_scores = dense_scores / dense_scores.max()
    
    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
    hybrid_scores = alpha * bm25_scores + (1 - alpha) * dense_scores
    
    # 5. Top-k ì„ íƒ
    top_indices = np.argsort(-hybrid_scores)[:k]
    top_scores = hybrid_scores[top_indices]
    top_chunks = [store.chunks[i] for i in top_indices]
    
    return top_indices.tolist(), top_scores.tolist(), top_chunks


# =========================================================
# íŒŒì´í”„ë¼ì¸
# =========================================================
def cmd_ingest(ns: argparse.Namespace):
    path = Path(ns.path)
    if not path.exists():
        raise SystemExit(f"[ERROR] File not found: {path}")

    raw = read_text(path)
    chunks = make_chunks(raw, unit=ns.unit, window=ns.window, stride=ns.stride)
    if not chunks:
        raise SystemExit("[ERROR] ë¹ˆ ë¬¸ì„œ")

    print(f"[INFO] chunks: {len(chunks)} (unit={ns.unit})")

    # Dense ì„ë² ë”© + FAISS
    print("[INFO] Building dense embeddings...")
    vecs = embed_texts(chunks)
    dense_idx = build_faiss_index(vecs)

    # BM25 ì¸ë±ìŠ¤
    print("[INFO] Building BM25 index...")
    bm25 = build_bm25_index(chunks)

    # ì €ì¥
    base = STORAGE_ROOT / ns.doc_id
    base.mkdir(parents=True, exist_ok=True)
    
    save_faiss(dense_idx, base / "faiss.index")
    save_bm25(bm25, base / "bm25.pkl")

    store = RAGStore(
        doc_id=ns.doc_id, 
        chunks=chunks, 
        emb_dim=vecs.shape[1],
        index_path=base / "faiss.index",
        bm25_path=base / "bm25.pkl",
        meta_path=base / "meta.json"
    )
    store.save_meta()

    print(f"[OK] Ingested: {ns.doc_id} | chunks={len(chunks)} | dim={vecs.shape[1]}")

def build_answer_prompt(question: str, contexts: list[str]) -> str:
    ctx_joined = "\n\n---\n\n".join(contexts)
    return f"""ë„ˆëŠ” í•œêµ­ì–´ ë…ì„œ ë„ìš°ë¯¸ë‹¤. ì•„ë˜ 'ê·¼ê±° ë¬¸ë§¥'ë§Œ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ë¼.

ê·œì¹™:
- ë‹µë³€ì€ ğŸ“Œ ì´ëª¨ì§€ë¡œ ì‹œì‘í•˜ëŠ” 1ë¬¸ë‹¨ ìš”ì•½, ì²« ì¤„ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ **ì§ì ‘ ë‹µë³€** 
- ì´ì–´ì„œ ğŸ“ ì´ëª¨ì§€ë¡œ í•µì‹¬ ì¸ìš© 1ì¤„ë§Œ ë³´ì—¬ì¤˜ (ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°)
- ë¶ˆí™•ì‹¤í•˜ë©´ "ë³¸ë¬¸ì— ëª…í™•í•œ ê·¼ê±° ì—†ìŒ"ì´ë¼ê³  ë§í•´

[ì§ˆë¬¸]
{question}

[ê·¼ê±° ë¬¸ë§¥]
{ctx_joined}
"""

def cmd_ask(ns: argparse.Namespace):
    ids, scores, hits = hybrid_retrieve(ns.doc_id, ns.q, k=ns.k, alpha=ns.alpha)
    
    context_text = "\n\n---\n\n".join(hits)

    prompt = f"""
ë…ì„œ Q/A ê³¼ì œì…ë‹ˆë‹¤.
ì˜¤ë¡œì§€ ì•„ë˜ ë¬¸ë§¥ë§Œ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ë§¥]
{context_text}

[ì§ˆë¬¸]
{ns.q}


ê·œì¹™:
- ë‹µë³€ì€ ğŸ“Œ ì´ëª¨ì§€ë¡œ ì‹œì‘í•˜ëŠ” 1ë¬¸ë‹¨ ìš”ì•½, ì²« ì¤„ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ **ì§ì ‘ ë‹µë³€** 
- ì´ì–´ì„œ ğŸ“ ì´ëª¨ì§€ë¡œ í•µì‹¬ ì¸ìš© 1ì¤„ë§Œ ë³´ì—¬ì¤˜ (ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°)
- ë¶ˆí™•ì‹¤í•˜ë©´ "ë³¸ë¬¸ì— ëª…í™•í•œ ê·¼ê±° ì—†ìŒ"ì´ë¼ê³  ë§í•´
"""
    answer = gpt4omini_chat(prompt, max_tokens=400)

    print(f"\nâ“ {ns.q}\n")
    print(f"ğŸ§  ë‹µë³€:\n{answer}\n")


def cmd_summarize(ns: argparse.Namespace):
    store = RAGStore.load(ns.doc_id)
    text = "\n\n".join(store.chunks)
    
    # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë‚˜ëˆ ì„œ ìš”ì•½
    step = 1500
    parts = [text[i:i+step] for i in range(0, len(text), step)]
    
    partials = []
    for i, chunk in enumerate(parts, 1):
        p = f"""ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ 3~5ë¬¸ì¥ bullet ìš”ì•½:

{chunk}
"""
        partials.append(gpt4omini_chat(p, max_tokens=200))
    
    reduce_prompt = f"""ë‹¤ìŒ ë¶€ë¶„ ìš”ì•½ë“¤ì„ í†µí•©í•´ ìµœì¢… {ns.sentences}ë¬¸ì¥ ìš”ì•½ ì‘ì„±:

êµ¬ì„±:
- ğŸ“Œ í•œì¤„ ìš”ì•½ (1ë¬¸ì¥)
- âœ… ì¤„ê±°ë¦¬ í•µì‹¬ (ë²ˆí˜¸)
- ğŸ‘¥ ì£¼ìš” ì¸ë¬¼ ê´€ê³„
- ğŸ§  ì£¼ì œ/ì •ì„œ

ë¶€ë¶„ ìš”ì•½:
{chr(10).join('- ' + s for s in partials)}
"""
    final = gpt4omini_chat(reduce_prompt, max_tokens=400)
    print(final)

# =========================================================
# CLI
# =========================================================
def build_parser():
    ap = argparse.ArgumentParser(description="RAG MVP: Hybrid Search (BM25+Dense) + Llama")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ap_i = sub.add_parser("ingest")
    ap_i.add_argument("--doc_id", required=True)
    ap_i.add_argument("--path", required=True)
    ap_i.add_argument("--unit", choices=["para","sent"], default="para")
    ap_i.add_argument("--window", type=int, default=1)
    ap_i.add_argument("--stride", type=int, default=1)
    ap_i.set_defaults(func=cmd_ingest)

    ap_a = sub.add_parser("ask")
    ap_a.add_argument("--doc_id", required=True)
    ap_a.add_argument("-q", required=True)
    ap_a.add_argument("-k", type=int, default=6)
    ap_a.add_argument("--alpha", type=float, default=0.5, 
                     help="BM25 ê°€ì¤‘ì¹˜ (0~1). 0.5=ê· í˜•, 0.7=í‚¤ì›Œë“œ ì¤‘ì‹œ, 0.3=ì˜ë¯¸ ì¤‘ì‹œ")
    ap_a.set_defaults(func=cmd_ask)

    ap_s = sub.add_parser("summarize")
    ap_s.add_argument("--doc_id", required=True)
    ap_s.add_argument("--sentences", type=int, default=7)
    ap_s.set_defaults(func=cmd_summarize)

    return ap



# =========================================================
# Gradio UI
# =========================================================
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
def gpt4omini_chat(prompt: str, max_tokens=300):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.2,
            top_p=0.9,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ERROR: GPT-4o-mini] {e}"

def run_gradio():
    import gradio as gr
    from pathlib import Path

    # ------------------------
    # 1) Functions (ì •ì˜ ë¨¼ì €)
    # ------------------------

    # ---- ingest ----
    def ui_ingest(doc_id, file_obj, unit, window, stride):
        if file_obj is None:
            return "[ERROR] í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."

        try:
            # ------------------------------
            # 1) file_objê°€ bytesì¸ ê²½ìš° (Gradio type="binary")
            # ------------------------------
            if isinstance(file_obj, bytes):
                try:
                    text = file_obj.decode("utf-8")
                except UnicodeDecodeError:
                    text = file_obj.decode("cp949")  # í•œê¸€ ìœˆë„ìš° txt ëŒ€ì‘

            # ------------------------------
            # 2) file_objê°€ dict í˜•íƒœì¸ ê²½ìš° (data í•„ë“œ ì¡´ì¬)
            # ------------------------------
            elif isinstance(file_obj, dict):
                data = file_obj.get("data")
                if data is None:
                    return "[ERROR] ì—…ë¡œë“œëœ íŒŒì¼ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
                try:
                    text = data.decode("utf-8")
                except UnicodeDecodeError:
                    text = data.decode("cp949")

            # ------------------------------
            # 3) file_objê°€ tempíŒŒì¼ ê°ì²´ì¸ ê²½ìš°
            # ------------------------------
            elif hasattr(file_obj, "read"):
                raw = file_obj.read()
                try:
                    text = raw.decode("utf-8")
                except UnicodeDecodeError:
                    text = raw.decode("cp949")

            else:
                return "[ERROR] ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."

            # ------------------------------
            # 4) ë‚´ìš©ì´ ì§„ì§œ ë¹„ì—ˆëŠ”ì§€ í™•ì¸
            # ------------------------------
            if len(text.strip()) == 0:
                return "[ERROR] ì—…ë¡œë“œëœ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

            # ------------------------------
            # 5) í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            # ------------------------------
            temp_path = Path("temp_upload.txt")
            temp_path.write_text(text, encoding="utf-8")

        except Exception as e:
            return f"[ERROR] íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

        # ingest ì‹¤í–‰
        class Ns: pass
        ns = Ns()
        ns.doc_id = doc_id
        ns.path = str(temp_path)
        ns.unit = unit
        ns.window = window
        ns.stride = stride

        try:
            cmd_ingest(ns)
            return f"[OK] {doc_id} ingest ì™„ë£Œ!"
        except Exception as e:
            return f"[ERROR] {e}"



    # ---- ask ----
    def ui_ask(doc_id, question, k, alpha):
        try:
            ids, scores, chunks = hybrid_retrieve(doc_id, question, k=k, alpha=alpha)
            ctx = "\n\n---\n\n".join(chunks)

            prompt = f"""
    ë…ì„œ Q/A ê³¼ì œì…ë‹ˆë‹¤.
    ì˜¤ë¡œì§€ ì•„ë˜ ë¬¸ë§¥ë§Œ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

    [ë¬¸ë§¥]
    {ctx}

    [ì§ˆë¬¸]
    {question}

    ì¶œë ¥ í˜•ì‹:
    - ì²« ë¬¸ë‹¨: ì •ë‹µ (ì§ì ‘)
    - ë‹¤ìŒ: 2~4ê°œ bullet ê·¼ê±° ìš”ì•½
    - ë§ˆì§€ë§‰: í•µì‹¬ ì¸ìš© 1ì¤„(ë”°ì˜´í‘œ)
    """

            answer = gpt4omini_chat(prompt, max_tokens=400)

            preview = "\n\n".join(
                [f"[{i+1}] score={scores[i]:.3f}\n{chunks[i][:200]}"
                for i in range(len(chunks))]
            )
            return answer, preview

        except Exception as e:
            return f"[ERROR] {e}", ""


    # ---- summarize ----
    # def ui_summarize(doc_id, sentences):
    #     try:
    #         store = RAGStore.load(doc_id)
    #         text = "\n\n".join(store.chunks)
    #         step = 5000
    #         parts = [text[i:i + step] for i in range(0, len(text), step)]
    #         partials = []
    #         for ch in parts:
    #             p = f"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ìš”ì•½:\n{ch}"
    #             partials.append(llama_chat(p, max_new_tokens=200))

    #         reduce_prompt = (
    #             f"ë‹¤ìŒ ë¶€ë¶„ ìš”ì•½ì„ {sentences}ë¬¸ì¥ìœ¼ë¡œ í†µí•©:\n" +
    #             "\n".join('- ' + s for s in partials)
    #         )
    #         return llama_chat(reduce_prompt)

    #     except Exception as e:
    #         return f"[ERROR] {e}"
    def ui_summarize(doc_id, sentences):
        try:
            store = RAGStore.load(doc_id)
            text = "\n\n".join(store.chunks)

            # ê¸¸ê²Œ ìª¼ê°œê¸°
            step = 5000
            parts = [text[i:i + step] for i in range(0, len(text), step)]

            partials = []
            for ch in parts:
                prompt = f"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ìš”ì•½:\n{ch}"
                mini = gpt4omini_chat(prompt, max_tokens=300)
                partials.append(mini)

            reduce_prompt = (
                f"ë‹¤ìŒ ë¶€ë¶„ ìš”ì•½ì„ {sentences}ë¬¸ì¥ìœ¼ë¡œ í†µí•©:\n" +
                "\n".join('- ' + s for s in partials)
            )

            final = gpt4omini_chat(reduce_prompt, max_tokens=300)
            return final

        except Exception as e:
            return f"[ERROR] {e}"

    # -----------------------------------
    # 2) Gradio UI Layout (í•¨ìˆ˜ ì•„ë˜ì—)
    # -----------------------------------

    with gr.Blocks(title="ReadMate RAG MVP (Hybrid Search)") as demo:
        gr.Markdown("# ğŸ“š ReadMate RAG MVP (Hybrid Search)")

        # ---- ingest tab ----
        with gr.Tab("ğŸ“¥ Ingest"):
            doc_id = gr.Textbox(label="Doc ID")
            file_upload = gr.File(label="Upload TXT File", type="binary")
            unit = gr.Radio(["para", "sent"], value="para", label="Chunk unit")
            window = gr.Slider(1, 5, value=1, label="Window")
            stride = gr.Slider(1, 5, value=1, label="Stride")
            ingest_btn = gr.Button("Ingest!")
            ingest_out = gr.Textbox(label="Result")
            ingest_btn.click(ui_ingest,
                             [doc_id, file_upload, unit, window, stride],
                             ingest_out)

        # ---- ask tab ----
        with gr.Tab("â“ Ask"):
            doc_id2 = gr.Textbox(label="Doc ID")
            question = gr.Textbox(label="Question")
            k = gr.Slider(1, 10, value=6, label="k (chunks)")
            alpha = gr.Slider(0.0, 1.0, value=0.5, label="alpha")
            ask_btn = gr.Button("Ask!")
            answer_out = gr.Textbox(label="LLM Answer")
            passage_out = gr.Textbox(label="Retrieved Chunks Preview")
            ask_btn.click(ui_ask, [doc_id2, question, k, alpha],
                          [answer_out, passage_out])

        # ---- summarize tab ----
        with gr.Tab("ğŸ“ Summarize"):
            doc_id3 = gr.Textbox(label="Doc ID")
            sent_num = gr.Slider(3, 20, value=7, label="Summary sentences")
            sum_btn = gr.Button("Summarize!")
            sum_out = gr.Textbox(label="Summary")
            sum_btn.click(ui_summarize, [doc_id3, sent_num], sum_out)

    demo.launch(server_name="127.0.0.1", share=False)



if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "ui":
        run_gradio()  # â† ê·¸ëƒ¥ ì´ë ‡ê²Œ ì‹¤í–‰
    else:
        parser = build_parser()
        ns = parser.parse_args()
        ns.func(ns)

